---
layout: page
current: page
cover:  assets/built/images/logo-python.png
navigation: True
title: 머신러닝 프로그래밍 12주차 - Binary classification with data Logistic regrsssion
tags: [python]  
class: post-template
subclass: 'post tag-python'
author: SeongJae Yu
sitemap:
lastmod: 2021-11-25
priority: 0.7
changefreq: 'monthly'
exclude: 'yes'
---

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```


```python
df1 = pd.read_csv("cat1.csv")
df0 = pd.read_csv("cat0.csv")
df1
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>entry</th>
      <th>event</th>
      <th>En1</th>
      <th>En2</th>
      <th>dt</th>
      <th>dr</th>
      <th>x1</th>
      <th>y1</th>
      <th>z1</th>
      <th>x2</th>
      <th>y2</th>
      <th>z2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>1.315638</td>
      <td>1.804403</td>
      <td>151.967212</td>
      <td>42.143105</td>
      <td>-1304.894054</td>
      <td>478.462799</td>
      <td>1544.333537</td>
      <td>-1157.361908</td>
      <td>606.067041</td>
      <td>1766.707850</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13</td>
      <td>4.827913</td>
      <td>2.283814</td>
      <td>161.020364</td>
      <td>1309.103187</td>
      <td>-117.835790</td>
      <td>-1464.457077</td>
      <td>69.425318</td>
      <td>-70.148754</td>
      <td>-383.723470</td>
      <td>-475.833903</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>16</td>
      <td>3.011821</td>
      <td>2.606455</td>
      <td>72.997202</td>
      <td>903.456139</td>
      <td>1593.153637</td>
      <td>-1100.580348</td>
      <td>805.977081</td>
      <td>1092.321506</td>
      <td>-1257.393260</td>
      <td>215.991954</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>22</td>
      <td>3.109662</td>
      <td>1.619302</td>
      <td>95.681674</td>
      <td>246.310774</td>
      <td>1414.147550</td>
      <td>300.466078</td>
      <td>2036.144245</td>
      <td>1510.664013</td>
      <td>317.401116</td>
      <td>2038.324112</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>37</td>
      <td>2.795250</td>
      <td>1.665277</td>
      <td>85.430204</td>
      <td>452.633419</td>
      <td>-1185.771203</td>
      <td>-1586.241286</td>
      <td>-921.906685</td>
      <td>-1048.390809</td>
      <td>-1585.937883</td>
      <td>-1129.581104</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9949</th>
      <td>9949</td>
      <td>99956</td>
      <td>1.499017</td>
      <td>2.253849</td>
      <td>174.517405</td>
      <td>938.596974</td>
      <td>-1091.980671</td>
      <td>1203.741862</td>
      <td>-840.761911</td>
      <td>-1055.791096</td>
      <td>574.858610</td>
      <td>-1024.616952</td>
    </tr>
    <tr>
      <th>9950</th>
      <td>9950</td>
      <td>99957</td>
      <td>2.705410</td>
      <td>2.874333</td>
      <td>715.772076</td>
      <td>175.621626</td>
      <td>950.171373</td>
      <td>898.111879</td>
      <td>-1897.760471</td>
      <td>962.544864</td>
      <td>677.295582</td>
      <td>-1880.154811</td>
    </tr>
    <tr>
      <th>9951</th>
      <td>9951</td>
      <td>99983</td>
      <td>2.943438</td>
      <td>2.512821</td>
      <td>1179.579896</td>
      <td>60.482252</td>
      <td>-1437.391692</td>
      <td>807.265834</td>
      <td>1213.002043</td>
      <td>-1571.892213</td>
      <td>721.972273</td>
      <td>1238.615481</td>
    </tr>
    <tr>
      <th>9952</th>
      <td>9952</td>
      <td>99987</td>
      <td>3.082408</td>
      <td>2.868833</td>
      <td>145.649583</td>
      <td>494.576245</td>
      <td>-1658.602340</td>
      <td>-479.454709</td>
      <td>-1472.514450</td>
      <td>-1619.486255</td>
      <td>-669.457050</td>
      <td>-931.362227</td>
    </tr>
    <tr>
      <th>9953</th>
      <td>9953</td>
      <td>99993</td>
      <td>3.196240</td>
      <td>1.490613</td>
      <td>43.678549</td>
      <td>213.871359</td>
      <td>1165.018820</td>
      <td>1386.749830</td>
      <td>-722.045299</td>
      <td>1158.905657</td>
      <td>1062.491877</td>
      <td>-889.901928</td>
    </tr>
  </tbody>
</table>
<p>9954 rows × 12 columns</p>
</div>




```python
plt.hist(df0["dt"], bins=100, density=True, histtype="step");
plt.hist(df1["dt"], bins=100, density=True, histtype="step");
```



![output_2_0](./img/machinelearning/12/output_2_0.png)




```python
#density - 확률 밀도를 설정하기 위해 가중치 데이터를 정규화한다

plt.hist(df0["y1"]-df0["y2"], bins=100, range=(-2000,2000), density=True, histtype="step");
plt.hist(df1["y1"]-df1["y2"], bins=100, range=(-2000,2000), density=True, histtype="step");
```



![output_3_0](./img/machinelearning/12/output_3_0.png)




```python
plt.hist2d(df0["dt"], df0["y1"]-df0["y2"]);
```



![output_4_0](./img/machinelearning/12/output_4_0.png)




```python
plt.hist2d(df1["dt"], df1["y1"]-df1["y2"]);
```



![output_5_0](./img/machinelearning/12/output_5_0.png)




```python
df0 = df0[:9000]
df1 = df1[:9000]
```


```python
# plt.plot 옵션 - https://namyoungkim.github.io/matplotlib/visualization/2017/10/06/visualization/

#markersize	(ms)->마커 크기

plt.plot(df0["dt"], df0["dr"], '.', ms=0.5)
plt.plot(df1["dt"], df1["dr"], '.', ms=0.5)
```




    [<matplotlib.lines.Line2D at 0x7f4cafacce10>]





![output_7_1](./img/machinelearning/12/output_7_1.png)




```python
plt.hist(4*df0["dt"]+df0["dr"], histtype="step")
plt.hist(4*df1["dt"]+df1["dr"], histtype="step")
```




    (array([3.766e+03, 3.191e+03, 1.252e+03, 4.780e+02, 1.930e+02, 7.500e+01,
            3.300e+01, 6.000e+00, 4.000e+00, 2.000e+00]),
     array([  15.20712953,  810.57056653, 1605.93400353, 2401.29744052,
            3196.66087752, 3992.02431452, 4787.38775152, 5582.75118852,
            6378.11462551, 7173.47806251, 7968.84149951]),
     <a list of 1 Patch objects>)





![output_8_1](./img/machinelearning/12/output_8_1.png)




```python
#시그 모이 드 함수는 수학적 로지스틱 함수입니다. 통계, 오디오 신호 처리, 생화학 및 인공 뉴런의 활성화 기능에 일반적으로 사용됩니다. 시그 모이 드 함수의 공식은F(x) = 1/(1 + e^(-x))입니다.
def sigmoid(z):
  return 1/(1+np.exp(-z))
```


```python
def f(x, y, a, b, c):
  return a*x + b*y + c
```


```python
dts1 = np.array(df1['dt'])/1000
dts0 = np.array(df0['dt'])/1000
drs1 = np.array(df1['dr'])/1000
drs0 = np.array(df0['dr'])/1000
label1 = np.ones(dts1.shape)
label0 = np.zeros(dts0.shape)
```


```python
dts = np.concatenate([dts1, dts0])
drs = np.concatenate([drs1, dts0])
label = np.concatenate([label1, label0])
data = np.stack([dts, drs, label], axis=1) #가로축으로 합쳐진다
np.random.shuffle(data)
```


```python
data
```




    array([[0.38524939, 0.06985145, 1.        ],
           [0.03403795, 0.31482386, 1.        ],
           [0.01535261, 0.25602788, 1.        ],
           ...,
           [0.56028966, 0.56028966, 0.        ],
           [0.3714466 , 0.3714466 , 0.        ],
           [0.28964576, 0.28964576, 0.        ]])




```python
def gradF(x, y, a, b, c):
  # 1/(1+np.exp(-z))
  z = f(x, y, a, b, c)
  yy = sigmoid(z)*(1-sigmoid(z))
  # a*x + b*y + c
  da = yy*x
  db = yy*y
  dc = yy*1

  return da, db, dc
```


```python
losses = []
fitA, fitB, fitC = 1, 1, 1
batchSize = 16
eta = 1e-3
for epoch in range(10000):
  sumLoss = 0.
  nBatch = 0
  for batch in range(0, len(data), batchSize):
    ### Compute loss
    x = data[batch:batch+batchSize+1].T  #DataFrame.transform()는DataFrame에 함수를 적용하고 DataFrame을 변환합니다.
    dt, dr, label = x[0], x[1], x[2]
    z = f(dt, dr, fitA, fitB, fitC)
    z = sigmoid(z)

    df = label - z
    loss = (df**2).mean()

    ### Gradient
    #grad = -2*df*gradF(dt, dr, fitA, fitB, fitC)
    gradA, gradB, gradC = gradF(dt, dr, fitA, fitB, fitC)
    gradA = (df*gradA).mean()
    gradB = (df*gradB).mean()
    gradC = (df*gradC).mean()

    ### parameter update
    fitA = fitA + gradA*eta
    fitB = fitB + gradB*eta
    fitC = fitC + gradC*eta

    ### loss monitor
    #losses.append(loss)
    sumLoss += loss
    nBatch += 1
  losses.append(sumLoss/nBatch)  

```


```python
plt.plot(losses, '.-');
plt.yscale('log')
plt.xscale('log')
```



![output_16_0](./img/machinelearning/12/output_16_0.png)




```python
data0 = data[data[:,2]==0]
data1 = data[data[:,2]==1]
```


```python
plt.hist(sigmoid(f(data0.T[0], data0.T[1], fitA, fitB, fitC)), histtype='step')
plt.hist(sigmoid(f(data1.T[0], data1.T[1], fitA, fitB, fitC)), histtype='step')
```




    (array([ 101.,  133.,  146.,  177.,  260.,  320.,  527.,  900., 1776.,
            4660.]),
     array([8.41085958e-04, 9.27254960e-02, 1.84609906e-01, 2.76494316e-01,
            3.68378726e-01, 4.60263136e-01, 5.52147546e-01, 6.44031956e-01,
            7.35916366e-01, 8.27800777e-01, 9.19685187e-01]),
     <a list of 1 Patch objects>)





![output_18_1](./img/machinelearning/12/output_18_1.png)




```python
plt.plot(f(data.T[0], data.T[1], fitA, fitB, fitC), data.T[2], '.', ms=10, alpha=0.01)
#plt.hist(f(data1.T[0], data1.T[1], fitA, fitB, fitC)), histtype='step')
```




    [<matplotlib.lines.Line2D at 0x7f4cac627850>]





![output_19_1](./img/machinelearning/12/output_19_1.png)

  


```python

```
